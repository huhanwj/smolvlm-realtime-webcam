<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Teleoperation Interface (Text-Only)</title>
    <style>
        :root {
            --control-bg: rgba(0, 0, 0, 0.6);
            --text-color: #ffffff;
            --accent-color: #4CAF50;
            --stop-color: #f44336;
        }

        body, html {
            margin: 0;
            padding: 0;
            width: 100%;
            height: 100%;
            background-color: #000;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
            overflow: hidden; /* Prevent scrollbars */
        }

        #main-container {
            position: relative;
            width: 100vw;
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        #videoFeed, #guidanceCanvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover; /* Fill screen without stretching */
        }

        #guidanceCanvas {
            z-index: 10;
            pointer-events: none; /* Allow clicks to pass through to video */
        }

        .overlay {
            position: absolute;
            z-index: 20;
            padding: 15px;
            background: var(--control-bg);
            color: var(--text-color);
            border-radius: 12px;
            box-shadow: 0 4px 10px rgba(0,0,0,0.4);
            display: flex;
            flex-direction: column;
            gap: 10px;
        }

        #top-controls {
            top: 20px;
            left: 50%;
            transform: translateX(-50%);
            flex-direction: row;
            align-items: center;
        }

        #bottom-hud {
            bottom: 20px;
            left: 20px;
            right: 20px;
            max-width: 90vw;
            margin: 0 auto;
        }
        
        #responseText {
            font-size: 1.5em;
            font-weight: bold;
            color: var(--text-color);
            background: transparent;
            border: none;
            resize: none;
            text-align: center;
            width: 100%;
            min-height: 50px;
            padding: 10px 0;
        }

        #instruction-controls {
            display: flex;
            gap: 10px;
        }

        #instructionText {
            flex-grow: 1;
            padding: 10px;
            border-radius: 8px;
            border: 1px solid #555;
            background-color: #333;
            color: var(--text-color);
        }
        
        button {
            padding: 10px 20px;
            font-size: 16px;
            cursor: pointer;
            border: none;
            border-radius: 8px;
            color: white;
            transition: background-color 0.3s;
        }

        #startButton.start { background-color: var(--accent-color); }
        #startButton.start:hover { background-color: #66bb6a; }
        #startButton.stop { background-color: var(--stop-color); }
        #startButton.stop:hover { background-color: #ef5350; }

        .hidden {
            display: none;
        }
    </style>
</head>
<body>

    <div id="main-container">
        <video id="videoFeed" autoplay playsinline></video>
        <canvas id="guidanceCanvas"></canvas>
        <canvas id="captureCanvas" class="hidden"></canvas>
    </div>

    <div id="top-controls" class="overlay">
        <label for="baseURL">API URL:</label>
        <input id="baseURL" type="text" value="http://localhost:8080" style="width: 200px; padding: 5px; border-radius: 5px; border: none;">
        <button id="startButton" class="start">Start</button>
    </div>

    <div id="bottom-hud" class="overlay">
        <textarea id="responseText" name="Response" readonly placeholder="VLM guidance will appear here..."></textarea>
        <div id="instruction-controls">
            <textarea id="instructionText" name="Instruction" style="height: 40px;"></textarea>
        </div>
    </div>

    <script>
        // DOM Elements
        const video = document.getElementById('videoFeed');
        const guidanceCanvas = document.getElementById('guidanceCanvas');
        const captureCanvas = document.getElementById('captureCanvas');
        const baseURLInput = document.getElementById('baseURL');
        const instructionText = document.getElementById('instructionText');
        const responseText = document.getElementById('responseText');
        const startButton = document.getElementById('startButton');

        // State variables
        let stream;
        let idleTimer;
        let debounceTimer;
        let isProcessing = false;
        let chatHistory = [];
        const SYSTEM_PROMPT = "You are a teleoperation assistant. Describe the scene and guide the user. If you see a clear object of interest based on the user's request, provide a 'direction' (left, right, center, up, down) and a 'text' description in a JSON object.";
        
        // --- Core Functions ---

        /**
         * Initializes camera and sets up canvas dimensions.
         */
        async function initCamera() {
            try {
                stream = await navigator.mediaDevices.getUserMedia({ 
                    video: { facingMode: 'environment' }
                });
                video.srcObject = stream;
                video.onloadedmetadata = () => {
                    resizeCanvas();
                };
                responseText.value = "Camera ready. Press Start.";
            } catch (err) {
                console.error("Error accessing camera:", err);
                responseText.value = `Camera Error: ${err.message}. Ensure permissions are granted.`;
                alert(`Camera Error: ${err.message}. Please grant camera permissions.`);
            }
        }

        /**
         * Sends a condensed history and current image to the VLM.
         * @param {string} instruction The user's latest instruction.
         * @param {string} imageBase64URL The base64 encoded image data.
         * @returns {Promise<string>} The VLM's response text.
         */
        async function sendChatCompletionRequest(instruction, imageBase64URL) {
            const lastAssistantMessage = chatHistory.length > 0 ? chatHistory[chatHistory.length - 1] : null;

            // Construct a concise message list to save context window
            const messages = [
                { role: 'system', content: SYSTEM_PROMPT },
            ];
            if (lastAssistantMessage) {
                messages.push(lastAssistantMessage); // Add last VLM response for context
            }
            messages.push({
                role: 'user',
                content: [
                    { type: 'text', text: instruction },
                    { type: 'image_url', image_url: { url: imageBase64URL } }
                ]
            });
            
            try {
                const response = await fetch(`${baseURLInput.value}/v1/chat/completions`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        max_tokens: 150,
                        messages: messages // Send the condensed list
                    })
                });

                if (!response.ok) {
                    const errorData = await response.text();
                    throw new Error(`Server error ${response.status}: ${errorData}`);
                }

                const data = await response.json();
                const assistantResponse = data.choices[0].message.content;

                // Save only the last assistant response to history
                chatHistory = [{ role: 'assistant', content: assistantResponse }];

                return assistantResponse;
            } catch (error) {
                console.error("API Request Error:", error);
                return `Error: ${error.message}`;
            }
        }

        /**
         * Captures a frame from the video feed.
         * @returns {string|null} Base64 encoded JPEG image or null on failure.
         */
        function captureImage() {
            if (!stream || !video.videoWidth) {
                console.warn("Video stream not ready for capture.");
                return null;
            }
            const context = captureCanvas.getContext('2d');
            context.drawImage(video, 0, 0, captureCanvas.width, captureCanvas.height);
            return captureCanvas.toDataURL('image/jpeg', 0.7);
        }
        
        /**
         * The main loop for capturing, sending, and processing data.
         */
        async function processFrame() {
            if (!isProcessing) return;

            const instruction = instructionText.value;
            if (!instruction) {
                responseText.value = "Please provide an instruction to start.";
                // Don't stop, just wait for instruction
                return;
            }
            
            const imageBase64URL = captureImage();
            if (!imageBase64URL) {
                responseText.value = "Failed to capture image.";
                return;
            }

            try {
                const response = await sendChatCompletionRequest(instruction, imageBase64URL);
                handleVLMResponse(response);
            } catch (error) {
                console.error('Error processing frame:', error);
                responseText.value = `Error: ${error.message}`;
            } finally {
                // After any processing, reset the idle timer
                resetIdleTimer();
            }
        }
        
        // --- UI and Event Handlers ---

        function handleStart() {
            if (!stream) {
                alert("Camera not available. Cannot start.");
                return;
            }
            isProcessing = true;
            chatHistory = []; // Reset conversation
            startButton.textContent = "Stop";
            startButton.classList.replace('start', 'stop');
            baseURLInput.disabled = true;
            
            instructionText.value = "Describe what you see and look for a person.";
            responseText.value = "Starting session...";
            
            processFrame(); // Start processing immediately once
        }

        function handleStop() {
            isProcessing = false;
            clearTimeout(idleTimer);
            clearTimeout(debounceTimer);
            startButton.textContent = "Start";
            startButton.classList.replace('stop', 'start');
            baseURLInput.disabled = false;
            
            if(responseText.value.startsWith("Starting session...")) {
                responseText.value = "Session stopped.";
            }
            clearGuidance();
        }

        startButton.addEventListener('click', () => {
            isProcessing ? handleStop() : handleStart();
        });

        function handleVLMResponse(response) {
            try {
                const responseJson = JSON.parse(response);
                responseText.value = responseJson.text || "Received structured response without text.";
                drawGuidance(responseJson.direction);
            } catch (e) {
                responseText.value = response;
                clearGuidance();
            }
        }

        function handleUserInput() {
            if (!isProcessing) return;
            clearTimeout(idleTimer);
            clearTimeout(debounceTimer);
            debounceTimer = setTimeout(() => {
                responseText.value = "Processing new instruction...";
                processFrame();
            }, 500); // 0.5s debounce after user stops typing
        }
        
        instructionText.addEventListener('input', handleUserInput);

        function resetIdleTimer() {
            if (!isProcessing) return;
            clearTimeout(idleTimer);
            clearTimeout(debounceTimer);
            idleTimer = setTimeout(() => {
                processFrame();
            }, 2000); // 2-second idle interval (0.5Hz)
        }
        
        // --- Visual Guidance ---
        
        function drawGuidance(direction) {
            const ctx = guidanceCanvas.getContext('2d');
            ctx.clearRect(0, 0, guidanceCanvas.width, guidanceCanvas.height);
            if (!direction) return;

            ctx.fillStyle = 'rgba(255, 255, 0, 0.8)';
            ctx.font = '10vh sans-serif';
            ctx.textAlign = 'center';
            ctx.textBaseline = 'middle';

            let arrow;
            switch(direction.toLowerCase()) {
                case 'left':  arrow = 'â¬…ï¸'; break;
                case 'right': arrow = 'âž¡ï¸'; break;
                case 'up':    arrow = 'â¬†ï¸'; break;
                case 'down':  arrow = 'â¬‡ï¸'; break;
                case 'center':arrow = 'ðŸŽ¯'; break;
                default:      return;
            }
            ctx.fillText(arrow, guidanceCanvas.width / 2, guidanceCanvas.height / 2);
        }

        function clearGuidance() {
            const ctx = guidanceCanvas.getContext('2d');
            ctx.clearRect(0, 0, guidanceCanvas.width, guidanceCanvas.height);
        }

        function resizeCanvas() {
            guidanceCanvas.width = video.clientWidth;
            guidanceCanvas.height = video.clientHeight;
            captureCanvas.width = video.videoWidth;
            captureCanvas.height = video.videoHeight;
        }

        window.addEventListener('resize', resizeCanvas);

        // --- Initialization ---

        window.addEventListener('DOMContentLoaded', () => {
            initCamera();
        });

        // Cleanup on page close
        window.addEventListener('beforeunload', () => {
            handleStop(); // Ensure timers are cleared
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
            }
        });

    </script>
</body>
</html>
